# AdEvaÂ FinalÂ âš¡ â€”Â Energyâ€‘Load ForecastingÂ &Â MLOps Pipeline

> Transformerâ€‘based forecasting with automated CIÂ +Â CD, experiment tracking, and cloudâ€‘agnostic deployment â€” built for the **Ausgrid** timeâ€‘series dataset and Windowsâ€‘friendly tooling.

---

## ğŸ”‘Â Key Features

| Module | What it does | Location |
|--------|--------------|----------|
| **DataÂ IngestionÂ &Â Transformation** | Downloads raw CSVs, cleans, normalises, and slides a fixed window over the series to create `(X,Â y)` pairs. | `src/AdEva_final/pipeline/data_ingestion.py`Â &Â `data_transformation.py` |
| **Timeâ€‘Series Transformer** | Lightweight encoder (multiâ€‘head selfâ€‘attention) with sinusoidal positional encodings. Outputs a 7â€‘day forecast. | `pipeline/model_trainer.py` |
| **Dualâ€‘Format Model Export** | After training, weights are saved as<br>â€¢Â `model.pt`Â (PyTorch `state_dict`)<br>â€¢Â `model.h5`Â (true HDFâ€‘5 â€” Kerasâ€‘/TFâ€‘compatible) | `models/` |
| **Experiment Tracking** | Hyperâ€‘params & metrics (loss/epoch) logged to MLflow **remote or local** backend. | `mlflow/` |
| **Continuousâ€¯Integration** | `ci.yml` (GitHubÂ Actions) creates the Conda env, runs unit tests, trains on a small subset, uploads `transformer-model` artifact. | `.github/workflows/ci.yml` |
| **Continuousâ€¯Deployment** | `cd.yml` downloads the artifact after every green CI run and pushes the fresh `transformer.h5` to any ofÂ â€”<br>1. another GitHub repo (PAT)<br>2. SMB / IIS folder<br>3. S3 / MinIO bucket | `.github/workflows/cd.yml` |

---

## ğŸ”§Â Prerequisites

* **PythonÂ â‰¥3.10**
* **CondaÂ 4.12+** (or Mamba for speed)
* GPU optional (CUDA 11.x); falls back to CPU
* MLflow server (any backend store; SQLite works for dev)

---

## ğŸš€Â QuickÂ Start

```bash
# 1Â â€”Â clone
$ git clone https://github.com/nutsfinder/AdEva_final.git && cd AdEva_final

# 2Â â€”Â create env & install deps
$ conda env create -n adeva_env -f environment.yml   # OR:
$ conda create -n adeva_env python=3.10 && conda activate adeva_env
$ pip install -r requirements.txt

# 3Â â€”Â preâ€‘process the dataset (oneâ€‘off)
$ python src/AdEva_final/pipeline/data_ingestion.py
$ python src/AdEva_final/pipeline/data_transformation.py

# 4Â â€”Â train & log to MLflow
$ python src/AdEva_final/pipeline/model_trainer.py
```

Open **MLflow UI**: `mlflow ui -h 0.0.0.0 -p 5000` â†’ http://localhost:5000

---

## ğŸ› Â ProjectÂ Structure

```
AdEva_final/
â”œâ”€â”€ .github/workflows/    # CIÂ &Â CD pipelines
â”œâ”€â”€ models/              # *.pt & *.h5 artefacts (gitâ€‘ignored)
â”œâ”€â”€ notebooks/           # exploratory notebooks
â”œâ”€â”€ src/AdEva_final/
â”‚   â”œâ”€â”€ config/          # centralised YAML/JSON configs
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ data_ingestion.py
â”‚       â”œâ”€â”€ data_transformation.py
â”‚       â””â”€â”€ model_trainer.py
â””â”€â”€ requirements.txt
```

---

## ğŸ¤–Â CI & CD in OneÂ Minute

1. **Commit / PR open** â†’ **CI** spins up âŸ trains âŸ uploads `transformer-model` artifact.
2. CI finishes green âŸ **CD** autoâ€‘fires (`workflow_run`) âŸ downloads artifact.
3. CD pushes `transformer.h5` to the **destination** you configured (another repo, local share, S3â€¦).

Secrets used:

* `APP_PAT`Â Â â€” Personalâ€‘access token (repoâ€‘scope) for crossâ€‘repo pushes  *(or use `GITHUB_TOKEN` for sameâ€‘repo writes).*  
* Any cloud credentials (AWSÂ keys, SMB userâ€¦) stored the same way.

---

## ğŸ“¦Â Consuming the Model

```python
import torch, h5py
from AdEva_final.pipeline.model_trainer import TimeSeriesTransformer

with h5py.File("transformer.h5") as f:
    state = {k: torch.tensor(f[k][:]) for k in f}

model = TimeSeriesTransformer(feature_size=FEATURES)
model.load_state_dict(state)
model.eval()
forecast = model(input_window_tensor)
```

---

## ğŸ§ªÂ Testing & Smoke Checks

* **Unit tests**: `pytest -q tests/` (covers windowing logic and forward pass)
* **GitHubÂ Actions**: green badge â†’ every step succeeded, artefacts uploaded, model deployed.
* **Manual**: pull deployed `.h5`, run the snippet above, confirm `(batch,Â 7)` output.

---

## ğŸ™Â Contributing

PRs & issues welcome! Please:

1. Fork âŸ feature branch âŸ add tests.
2. Run `black` & `ruff`.
3. Verify `pytest` + local MLflow run pass.
4. Open PR; GitHubÂ Actions will comment with CIÂ +Â CD results.

---

## ğŸ“Â License

Â©Â 2025Â â€” NutsFinder. Released under the MIT License â€” see [`LICENSE`](LICENSE).

